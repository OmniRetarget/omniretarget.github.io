<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OmniRetarget</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="/static/css/bulma.min.css">
  <link rel="stylesheet" href="/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="/static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="/static/js/fontawesome.all.min.js"></script>
  <script src="/static/js/bulma-carousel.min.js"></script>
  <script src="/static/js/bulma-slider.min.js"></script>
  <script src="/static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-fullhd">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">OmniRetarget</h1>
            <h2 class="subtitle is-2 publication-title">Interaction-Preserving Data Generation for Humanoid<br>Whole-Body Loco-Manipulation and Scene Interaction</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank" href="https://lujieyang.github.io/">Lujie Yang<sup>*</sup></a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://scholar.google.com/citations?user=G-x_szsAAAAJ&hl=en">Xiaoyu Huang<sup>*</sup></a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://www.linkedin.com/in/zhen-wu-326a70230/">Zhen Wu<sup>*</sup></a><sup>1</sup>,
              </span>
              <br>
              <span class="author-block">
                <a target="_blank" href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa<sup>†</sup></a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel<sup>†</sup></a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://sferrazza.cc/">Carmelo Sferrazza<sup>†</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://tml.stanford.edu/people/karen-liu">C. Karen Liu<sup>†</sup></a><sup>1,4</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="http://rockyduan.com/">Rocky Duan<sup>†</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a target="_blank" href="https://www.gshi.me/">Guanya Shi<sup>†</sup></a><sup>1,5</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Amazon FAR (Frontier AI & Robotics),</span>
              <span class="author-block"><sup>2</sup>MIT,</span>
              <span class="author-block"><sup>3</sup>UC Berkeley,</span>
              <span class="author-block"><sup>4</sup>Stanford University,</span>
              <span class="author-block"><sup>5</sup>CMU</span>
            </div>

            <div class="is-size-6" style="margin-top: 0.5rem; font-style: italic;">
              <sup>*</sup> Equal contribution, work done while interning at Amazon FAR. <sup>†</sup> Amazon FAR team co-lead.
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" href="/static/images/paper.pdf"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

              <!-- Video Link. -->
              <span class="link-block">
                <a target="_blank" href="https://youtu.be/xGNN4soesag"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a target="_blank" href="https://huggingface.co/datasets/omniretarget/OmniRetarget_Dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github-alt"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://omniretarget.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="section" style="padding-top: 0rem; padding-bottom: 0rem;">
  <div class="container">
    <div class="has-text-centered">
      <video autoplay muted loop playsinline controls style="width: 100%; max-width: 1000px; border-radius: 10px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);">
        <source src="/static/website_videos/flagship.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
        <h2 class="title is-2" style="text-align: center;">Abstract</h2>
        <div class="content has-text-justified">
          <p class="abstract-text" style="font-size: 0.95rem; line-height: 1.5;">
            A dominant paradigm for teaching humanoid
            robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies.
            However, existing retargeting pipelines often struggle with
            the significant embodiment gap between humans and robots,
            producing physically implausible artifacts like foot-skating and
            penetration. More importantly, common retargeting methods
            neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation.
            To address this, we introduce OmniRetarget, an interaction-
            preserving data generation engine based on an interaction
            mesh that explicitly models and preserves the crucial spatial
            and contact relationships between an agent, the terrain, and
            manipulated objects. By minimizing the Laplacian deformation
            between the human and robot meshes while enforcing kinematic
            constraints, OmniRetarget generates kinematically feasible
            trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration
            to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by
            retargeting motions from OMOMO, LAFAN1, and our
            in-house MoCap datasets, generating over 9-hour trajectories
            that achieve better kinematic constraint satisfaction and contact
            preservation than widely used baselines. Such high-quality data
            enables proprioceptive RL policies to successfully execute long-
            horizon (up to 30 seconds) parkour and loco-manipulation skills
            on a Unitree G1 humanoid, trained with only 5 reward terms
            and simple domain randomization shared by all tasks, without
            any learning curriculum.
          </p>
        </div>
        <div style="text-align: center;">
          <img src="/static/images/pipeline.png" class="interpolation-image" style="width: 75%; height: auto;"/>
        </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>Page template borrowed from <a href="https://nerfies.github.io/"><span class="dnerf">Nerfies</span></a>, <a href="https://beyondmimic.github.io/">BeyondMimic</a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
